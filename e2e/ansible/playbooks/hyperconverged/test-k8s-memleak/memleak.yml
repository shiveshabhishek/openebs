# k8s-cassandra-pod.yml
# Description:  Testing memory resource utilization test with dd workload on OpenEBS JIVA volume.

###############################################################################################
#Test Steps:
#1. Check whether the OpenEBS components are deployed.
#2. Copy the test artifacts to k8s master.
#3. Create a storage volume
#4. Obtain the IP address of the controller pod.
#5. Discover the volume from one of the K8s nodes.
#6. Check if the application pod is up and running
#7. Create file system and start the dd workload.
#8. Copy the script to the K8s node.
#9. Check if the memory consumption crosses the threshold value.
#10. Perform cleanup of test artifacts.
###############################################################################################

- hosts: localhost

  vars_files:
   - memleak-vars.yml

  tasks:
   - block:

     - include: memleak-prereq.yml

     - name: 1) Check whether maya-apiserver pod is deployed
       include_tasks: "{{ansible_env.HOME}}/{{utils_path}}/deploy_check.yml"
       vars:
         ns: "{{ operator_ns }}"
         app: maya-api

     - name: 2a) Copy the volume claim to kube master
       copy:
         src: "{{ volume_def }}"
         dest: "{{ result_kube_home.stdout }}"
       delegate_to: "{{ groups['kubernetes-kubemasters'].0 }}"

     - name: 2b) Replace volume-claim name with test parameters
       replace:
         path: "{{ result_kube_home.stdout }}/{{ volume_def }}"
         regexp: 'vut'
         replace: '{{ volume_claim }}'
       delegate_to: "{{ groups['kubernetes-kubemasters'].0 }}"

     - name: Create test specific namespace
       shell: source ~/.profile; kubectl create ns {{ namespace }}
       args:
         executable: /bin/bash
       delegate_to: "{{groups['kubernetes-kubemasters'].0}}"

     - name: 3) Create a storage volume via a pvc
       include_tasks: "{{ansible_env.HOME}}/{{utils_path}}/deploy_task.yml"
       vars:
         app_yml: "{{ volume_def }}"
         ns: "{{ namespace }}"

     - name: 3a) Confirm volume container is running
       shell: source ~/.profile; kubectl get pods -n {{ namespace }} | grep pvc | grep {{item}} | grep Running | wc -l
       args:
         executable: /bin/bash
       register: result
       until: result.stdout|int >= 1
       delay: 30
       retries: 20
       with_items:
         - ctrl
         - rep
       delegate_to: "{{ groups['kubernetes-kubemasters'].0 }}"

     - name: 4) Get storage ctrl pod name
       shell: source ~/.profile; kubectl get pods -n {{ namespace }} | grep ctrl
       args:
         executable: /bin/bash
       register: ctrl_name
       delegate_to: "{{ groups['kubernetes-kubemasters'].0 }}"

     - name: 4a) Set ctrl pod name to variable
       set_fact:
         ctrl_pod_name: "{{ ctrl_name.stdout.split()[0] }}"

     - name: 4b) Get IP address of ctrl pod
       shell: source ~/.profile; kubectl get svc -n {{ namespace }} --no-headers | awk {'print $3'}
       args:
         executable: /bin/bash
       register: ctrl_IP
       delegate_to: "{{groups['kubernetes-kubemasters'].0}}"

     - name: 4c) Set IP of Pod to variable
       set_fact:
         ctrl_ip: "{{ ctrl_IP.stdout }}"

     - name: sleep for 60 seconds and continue with play
       wait_for:
         timeout: "60"

     - name: 5) Establish iSCSI session with volume
       shell: iscsiadm -m discovery -t st -p {{ ctrl_ip }}:3260
       args:
         executable: /bin/bash
       become: true
       delegate_to: "{{groups['kubernetes-kubeminions'].0}}"
       register: result
       util: "'0' in result.rc"
       delay: 30
       retries: 15

     - name: 5a) Store target iqn in variable
       set_fact:
         iqn: "{{result.stdout.split(',')[1].split()[1]}}"

     - name: 6) Login to iSCSI target
       open_iscsi:
         show_nodes: yes
         login: yes
         target: "{{iqn}}"
       become: true
       delegate_to: "{{groups['kubernetes-kubeminions'].0}}"
       register: result

     - name: 6a) Check device nodes
       set_fact:
         scsi_device: "{{result.devicenodes[0]}}"

     - name: 7) Create file system on iSCSI disk
       filesystem:
         fstype: ext4
         dev: "{{scsi_device}}"
         force: no
       become: true
       delegate_to: "{{groups['kubernetes-kubeminions'].0}}"

     - name: 7a) Mount device by Label
       mount:
         name: "{{mount_point}}"
         src: "{{scsi_device}}"
         fstype: ext4
         opts: discard,_netdev
         state: mounted
       become: true
       delegate_to: "{{groups['kubernetes-kubeminions'].0}}"

     - name: 7b) Place data using dd
       shell: |
          timeout 200s dd if=/dev/urandom of={{mount_point}}/f1 bs={{block_size[0]}} count={{count_num}} oflag=dsync &
          timeout 200s dd if=/dev/urandom of={{mount_point}}/f2 bs={{block_size[1]}} count={{count_num}} oflag=dsync &
       become: true
       delegate_to: "{{groups['kubernetes-kubeminions'].0}}"

     - name: 8) Copy the python script to kubemaster
       copy:
         src: "{{ py_file }}"
         dest: "{{ result_kube_home.stdout }}"
       delegate_to: "{{ groups['kubernetes-kubemasters'].0 }}"

     - name: 9) Run python script for memory usage validation it takes couple of minutes
       shell: source ~/.profile; python test-mem.py {{ namespace }}
       args:
         executable: /bin/bash
       delegate_to: "{{groups['kubernetes-kubemasters'].0}}"
       register: result
       failed_when: "'Pass' not in result.stdout"

     - name: Test Passed
       set_fact:
         flag: "Test Passed"
         status_id: 1

     - name: Set Status
       set_fact:
         status: "good"

     rescue:
       - name: Test Failed
         set_fact:
           flag: "Test Failed"
           status_id: 5

       - name: Set Status
         set_fact:
           status: "danger"

     always:
       - block:

           - include: memleak-cleanup.yml
 
           - name: Test Cleanup Passed
             set_fact:
               cflag: "Cleanup Passed"

         rescue:
           - name: Test Cleanup Failed
             set_fact:
               cflag: "Cleanup Failed"

         always:
           
           - name: Delete namespace
             shell: source ~/.profile; kubectl delete ns {{ namespace }}
             args:
               executable: /bin/bash
             delegate_to: "{{groups['kubernetes-kubemasters'].0}}"
             ignore_errors: true
 
           - include_tasks: "{{ansible_env.HOME}}/{{utils_path}}/stern_task.yml"
             vars:
               status: stop

           - name: Update Testrail
             shell: source ~/.profile; python3 {{ inventory_dir | dirname }}/files/updater.py -tuser {{ testrail_user }} -tpass {{ testrail_password }} -sid {{ test_suite_id }} -cid {{ test_case_id }} -stid {{ status_id }}
             args:
               executable: /bin/bash

           - name: Testrail Updated
             set_fact:
               tflag: "Testrail Updated"

         rescue:
           - name: Testrail Update failed
             set_fact:
               tflag: "Testrail Update Failed"

           - name: Send slack notification
             slack:
               token: "{{ lookup('env','SLACK_TOKEN') }}"
               msg: '{{ ansible_date_time.time }} TEST: {{test_name}}, RESULT: {{ flag }},{{ cflag }},{{tflag}}'
               color: "{{status}}"
             when: slack_notify | bool and lookup('env','SLACK_TOKEN')


# replica-node-affinity.yaml

#Description: Check if the pods are scheduled on the desired node only.

###############################################################################################
#Test Steps:
#1. Check if the OpenEBS components are deployed in openebs namespace.
#2. Obtain the storage class yaml from the deployment.
#3. Modify the replica count in storageclass yaml and create it.
#4. Deploy Percona application.
#5. Check if the application and the volume pods are up and running.
#6. Update the node names in the patch yaml.
#7. Patch the deployment to include node affinity spec.
#8. Delete the replica pod in one of the nodes and check if it gets scheduled on the same node again.
#9. Delete the test artifacts.
###############################################################################################


- hosts: localhost

  vars_files:
    - replica-node-affinity-vars.yml

  tasks:

   - block:

       - name: Start logger.
         include: prerequisistes.yml

# Checking if OpenEBS components are integrated successfully

       - name: 1) Check status of maya-api server
         include_tasks: "{{ansible_env.HOME}}/{{utils_path}}/deploy_check.yml"
         vars:
            ns: openebs
            app: apiserver

       - name: 2) Obtaining storage classes yaml
         shell: source ~/.profile; kubectl get sc openebs-percona -o yaml > "{{ create_sc }}"
         args:
           executable: /bin/bash
         delegate_to: "{{groups['kubernetes-kubemasters'].0}}"

       - name: 2b) Create test specific namespace
         shell: source ~/.profile; kubectl create ns "{{ namespace }}"
         args:
           executable: /bin/bash
         delegate_to: "{{groups['kubernetes-kubemasters'].0}}"

       - name: 2c) Get the number of nodes in the cluster
         shell: source ~/.profile; kubectl get nodes | grep '<none>' | wc -l
         args:
           executable: /bin/bash
         register: node_out
         delegate_to: "{{groups['kubernetes-kubemasters'].0}}"

       - name: 2d) Fetch the node count from stdout
         set_fact:
            node_count: " {{ node_out.stdout}}"

       - name: 3) Replace the replica count in storage classes yaml
         replace:
           path: "{{ create_sc }}"
           regexp: 'openebs.io/jiva-replica-count: "3"'
           replace: 'openebs.io/jiva-replica-count: "{{ (node_count) |int-1}}"'
         delegate_to: "{{groups['kubernetes-kubemasters'].0}}"

       - name: 3a) Delete the existing storage class and create new one
         shell: source ~/.profile; kubectl delete sc openebs-percona; sleep 10; kubectl apply -f "{{ create_sc }}"
         args:
           executable: /bin/bash
         register: sc_out
         delegate_to: "{{groups['kubernetes-kubemasters'].0}}"
         until: "'created' in sc_out.stdout"
         delay: 30
         retries: 5

       - name: 4) Deploying Percona Application
         shell: source ~/.profile; kubectl apply -f "{{ percona_link }}" -n "{{namespace}}"
         args:
           executable: /bin/bash
         register: app_out
         delegate_to: "{{groups['kubernetes-kubemasters'].0}}"
         until: "'created' in app_out.stdout"
         delay: 20
         retries: 6

       #Confirm if the application is running.

       - include_tasks: "{{ansible_env.HOME}}/{{utils_path}}/deploy_check.yml"
         vars:
            ns: "{{ namespace }}"
            app: percona

       - name: 5a) Check if the replica pods are created
         shell: source ~/.profile; kubectl get pods -n "{{namespace}}" | grep rep | grep -i running |wc -l
         args:
           executable: /bin/bash
         register: rep_count
         delegate_to: "{{groups['kubernetes-kubemasters'].0}}"
         until: "'2' in rep_count.stdout"
         delay: 60
         retries: 5
         ignore_errors: true

       # Getting PV name
       - name: 5b) Obtain PV name
         shell: source ~/.profile; kubectl get pv | grep openebs-percona | awk '{print $1}'
         args:
           executable: /bin/bash
         register: pv_name
         delegate_to: "{{groups['kubernetes-kubemasters'].0}}"

       - name: 5c) Copy replica_patch.yaml to master
         copy:
           src: "{{ patch_file }}"
           dest: "{{ result_kube_home.stdout }}"
         delegate_to: "{{groups['kubernetes-kubemasters'].0}}"

       - name: 6) Get the first node name where replica1 is scheduled
         shell: source ~/.profile; kubectl get po -n "{{ namespace }}" -o wide | grep "{{ pv_name.stdout }}" | grep rep | awk '{print $7}' | awk 'FNR == 1 {print}'
         args:
           executable: /bin/bash
         register: node1
         delegate_to: "{{groups['kubernetes-kubemasters'].0}}"

       - name: 6a) Get the second node name where replica1 is scheduled
         shell: source ~/.profile; kubectl get po -n "{{ namespace }}" -o wide | grep "{{ pv_name.stdout }}" | grep rep | awk '{print $7}' | awk 'FNR == 2 {print}'
         args:
           executable: /bin/bash
         register: node2
         delegate_to: "{{groups['kubernetes-kubemasters'].0}}"

       - name: 6b) Update first node name in replica_patch file
         replace:
           path: "{{ result_kube_home.stdout }}/{{ patch_file }}"
           regexp: 'nodename_where_replica_pod_1_got_scheduled'
           replace: '{{ node1.stdout }}'
         delegate_to: "{{groups['kubernetes-kubemasters'].0}}"

       - name: 6c) Update second node name in replica_patch file
         replace:
           path: "{{ result_kube_home.stdout }}/{{ patch_file }}"
           regexp: 'nodename_where_replica_pod_2_got_scheduled'
           replace: '{{ node2.stdout }}'
         delegate_to: "{{groups['kubernetes-kubemasters'].0}}"

       - name: 7) Obtain the deployment name
         shell: source ~/.profile; kubectl get deploy -n "{{ namespace }}" | grep "{{ pv_name.stdout }}" | grep rep | awk '{print $1}'
         args:
           executable: /bin/bash
         register: deploy_name
         delegate_to: "{{groups['kubernetes-kubemasters'].0}}"

       - name: 7a) Update the replica deployment with node affinity property
         shell: source ~/.profile; kubectl patch deployment "{{ deploy_name.stdout }}" -n "{{namespace}}" -p "$(cat {{ result_kube_home.stdout }}/replica_patch.yml)"
         args:
           executable: /bin/bash
         register: patch_out
         delegate_to: "{{groups['kubernetes-kubemasters'].0}}"
         until: "'patched' in patch_out.stdout"
         delay: 20
         retries: 5

       - name: Wait some time
         wait_for:
           timeout: 50

       - name: 8) Get pod name
         shell: source ~/.profile; kubectl get pods -n "{{ namespace }}" -o wide | grep "{{ pv_name.stdout }}" | grep rep | grep "{{ node1.stdout }}" | awk '{print $1}'
         args:
           executable: /bin/bash
         register: pod_name
         delegate_to: "{{groups['kubernetes-kubemasters'].0}}"

       - name: 8a) Delete the replica on one of the nodes
         shell: source ~/.profile; kubectl delete pod "{{ pod_name.stdout }}" -n "{{ namespace }}"
         args:
           executable: /bin/bash
         register: pod_delete
         delegate_to: "{{groups['kubernetes-kubemasters'].0}}"
         until: "'deleted' in pod_delete.stdout"
         delay: 60
         retries: 3

       - name: 8b) Check the available replicas in deployment
         shell: source ~/.profile; kubectl get deploy -n "{{ namespace }}" | grep "{{ pv_name.stdout }}" | grep rep | awk '{print $5}'
         args:
           executable: /bin/bash
         register: available_pods
         delegate_to: "{{groups['kubernetes-kubemasters'].0}}"
         until: "'2' in available_pods.stdout"
         delay: 30
         retries: 15

       - name: 8c) Check if the replica is scheduled again
         shell: source ~/.profile; kubectl get pods -n "{{namespace}}" -o wide | grep "{{ pv_name.stdout }}" | grep rep | grep Running | grep "{{ node1.stdout }}"
         args:
           executable: /bin/bash
         register: result
         delegate_to: "{{groups['kubernetes-kubemasters'].0}}"
         until: "'Running' in result.stdout"
         delay: 30
         retries: 15

       - name: Setting pass flag
         set_fact:
            flag: "Test Passed"
            status_id: 1

       - name: Set Status
         set_fact:
           status: "good"

     rescue:
       - name: Setting fail flag
         set_fact:
           flag: "Test Failed"
           status_id: 5

       - name: Set Status
         set_fact:
           status: "danger"

     always:

       - block:

           - include: replica-node-affinity-cleanup.yml

           - name: Test Cleanup Passed
             set_fact:
               cflag: "Cleanup Passed"

         rescue:
           - name: Test Cleanup Failed
             set_fact:
               cflag: "Cleanup Failed"

         always:

           - include_tasks: "{{ansible_env.HOME}}/{{utils_path}}/stern_task.yml"
             vars:
               status: stop

           - name: Update Testrail
             shell: source ~/.profile; python3 {{ inventory_dir | dirname }}/files/updater.py -tuser {{ testrail_user }} -tpass {{ testrail_password }} -sid {{ test_suite_id }} -cid {{ test_case_id }} -stid {{ status_id }}
             args:
               executable: /bin/bash

           - name: Testrail Updated
             set_fact:
               tflag: "Testrail Updated"

         rescue:
           - name: Testrail Update failed
             set_fact:
               tflag: "Testrail Update Failed"
           
           - name: Send slack notification
             slack:
               token: "{{ lookup('env','SLACK_TOKEN') }}"
               msg: '{{ ansible_date_time.time }} TEST: {{test_name}}, RESULT: {{ flag }},{{cflag}},{{tflag}}'
               color: "{{status}}"
             when: slack_notify | bool and lookup('env','SLACK_TOKEN')

